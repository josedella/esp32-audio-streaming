/*
 * Copyright (c) 2025 EdgeImpulse Inc.
 *
 * Generated by Edge Impulse and licensed under the applicable Edge Impulse
 * Terms of Service. Community and Professional Terms of Service
 * (https://edgeimpulse.com/legal/terms-of-service) or Enterprise Terms of
 * Service (https://edgeimpulse.com/legal/enterprise-terms-of-service),
 * according to your product plan subscription (the “License”).
 *
 * This software, documentation and other associated files (collectively referred
 * to as the “Software”) is a single SDK variation generated by the Edge Impulse
 * platform and requires an active paid Edge Impulse subscription to use this
 * Software for any purpose.
 *
 * You may NOT use this Software unless you have an active Edge Impulse subscription
 * that meets the eligibility requirements for the applicable License, subject to
 * your full and continued compliance with the terms and conditions of the License,
 * including without limitation any usage restrictions under the applicable License.
 *
 * If you do not have an active Edge Impulse product plan subscription, or if use
 * of this Software exceeds the usage limitations of your Edge Impulse product plan
 * subscription, you are not permitted to use this Software and must immediately
 * delete and erase all copies of this Software within your control or possession.
 * Edge Impulse reserves all rights and remedies available to enforce its rights.
 *
 * Unless required by applicable law or agreed to in writing, the Software is
 * distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
 * either express or implied. See the License for the specific language governing
 * permissions, disclaimers and limitations under the License.
 */
// Generated on: 02.12.2025 17:08:26

#include <stdio.h>
#include <stdlib.h>
#include "edge-impulse-sdk/tensorflow/lite/c/builtin_op_data.h"
#include "edge-impulse-sdk/tensorflow/lite/c/common.h"
#include "edge-impulse-sdk/tensorflow/lite/micro/micro_mutable_op_resolver.h"
#include "edge-impulse-sdk/porting/ei_classifier_porting.h"

#if EI_CLASSIFIER_PRINT_STATE
#if defined(__cplusplus) && EI_C_LINKAGE == 1
extern "C" {
    extern void ei_printf(const char *format, ...);
}
#else
extern void ei_printf(const char *format, ...);
#endif
#endif

#define STRINGIZE(x) #x
#define STRINGIZE_VALUE_OF(x) STRINGIZE(x)

#if defined (__GNUC__)  /* GNU compiler */
#define ALIGN(X) __attribute__((aligned(X)))
#define DEFINE_SECTION(x) __attribute__((section(x)))
#elif defined (_MSC_VER)
#define ALIGN(X) __declspec(align(X))
#elif defined (__TASKING__) /* TASKING Compiler */
#define ALIGN(X) __align(X)
#define DEFINE_SECTION(x) __attribute__((section(x)))
#elif defined (__ARMCC_VERSION) /* Arm Compiler */
#define ALIGN(X) __ALIGNED(x)
#define DEFINE_SECTION(x) __attribute__((section(x)))
#elif defined (__ICCARM__) /* IAR Compiler */
#define ALIGN(x) __attribute__((aligned(x)))
#define DEFINE_SECTION(x) __attribute__((section(x)))
#elif defined (__clang__) /* LLVM/Clang Compiler */
#define ALIGN(X) __ALIGNED(x)
#define DEFINE_SECTION(x) __attribute__((section(x)))
#endif

#if defined(EI_MODEL_SECTION) && (defined(__GNUC__) || defined(__clang__))
#define MODEL_SECTION(X) __attribute__((section(STRINGIZE_VALUE_OF(X))))
#else
#define MODEL_SECTION(X)
#endif

#ifndef EI_MAX_SCRATCH_BUFFER_COUNT
#ifndef CONFIG_IDF_TARGET_ESP32S3
#define EI_MAX_SCRATCH_BUFFER_COUNT 4
#else
#define EI_MAX_SCRATCH_BUFFER_COUNT 8
#endif // CONFIG_IDF_TARGET_ESP32S3
#endif // EI_MAX_SCRATCH_BUFFER_COUNT

#ifndef EI_MAX_OVERFLOW_BUFFER_COUNT
#define EI_MAX_OVERFLOW_BUFFER_COUNT 10
#endif // EI_MAX_OVERFLOW_BUFFER_COUNT

using namespace tflite;
using namespace tflite::ops;
using namespace tflite::ops::micro;

namespace {

#if defined(EI_CLASSIFIER_ALLOCATION_STATIC_HIMAX) || defined(EI_CLASSIFIER_ALLOCATION_STATIC_HIMAX_GNU)
constexpr int kTensorArenaSize = 10128;
#else
constexpr int kTensorArenaSize = 9104;
#endif

#if defined(EI_CLASSIFIER_ALLOCATION_STATIC)
#if defined (EI_TENSOR_ARENA_LOCATION)
uint8_t tensor_arena[kTensorArenaSize] ALIGN(16) DEFINE_SECTION(STRINGIZE_VALUE_OF(EI_TENSOR_ARENA_LOCATION));
#else
uint8_t tensor_arena[kTensorArenaSize] ALIGN(16);
#endif
#elif defined(EI_CLASSIFIER_ALLOCATION_STATIC_HIMAX)
#pragma Bss(".tensor_arena")
uint8_t tensor_arena[kTensorArenaSize] ALIGN(16);
#pragma Bss()
#elif defined(EI_CLASSIFIER_ALLOCATION_STATIC_HIMAX_GNU)
uint8_t tensor_arena[kTensorArenaSize] ALIGN(16) __attribute__((section(".tensor_arena")));
#else
#define EI_CLASSIFIER_ALLOCATION_HEAP 1
uint8_t* tensor_arena = NULL;
#endif

static uint8_t* tensor_boundary;
static uint8_t* current_location;

template <int SZ, class T> struct TfArray {
  int sz; T elem[SZ];
};

enum used_operators_e {
  OP_RESHAPE, OP_CONV_2D, OP_MAX_POOL_2D, OP_FULLY_CONNECTED, OP_SOFTMAX,  OP_LAST
};

struct TensorInfo_t { // subset of TfLiteTensor used for initialization from constant memory
  TfLiteAllocationType allocation_type;
  TfLiteType type;
  void* data;
  TfLiteIntArray* dims;
  size_t bytes;
  TfLiteQuantization quantization;
};

typedef struct {
  TfLiteTensor tensor;
  int16_t index;
} TfLiteTensorWithIndex;

typedef struct {
  TfLiteEvalTensor tensor;
  int16_t index;
} TfLiteEvalTensorWithIndex;

TfLiteContext ctx{};
static const int MAX_TFL_TENSOR_COUNT = 4;
static TfLiteTensorWithIndex tflTensors[MAX_TFL_TENSOR_COUNT];
static const int MAX_TFL_EVAL_COUNT = 4;
static TfLiteEvalTensorWithIndex tflEvalTensors[MAX_TFL_EVAL_COUNT];
TfLiteRegistration registrations[OP_LAST];

namespace g0 {
const TfArray<2, int> tensor_dimension0 = { 2, { 1,3960 } };
const TfArray<1, float> quant0_scale = { 1, { 0.0038756127469241619, } };
const TfArray<1, int> quant0_zero = { 1, { -128 } };
const TfLiteAffineQuantization quant0 = { (TfLiteFloatArray*)&quant0_scale, (TfLiteIntArray*)&quant0_zero, 0 };
const MODEL_SECTION(EI_MODEL_SECTION) ALIGN(16) int32_t tensor_data1[4] = { 1, 1, 99, 40, };
const TfArray<1, int> tensor_dimension1 = { 1, { 4 } };
const MODEL_SECTION(EI_MODEL_SECTION) ALIGN(16) int32_t tensor_data2[4] = { 1, 99, 1, 8, };
const MODEL_SECTION(EI_MODEL_SECTION) ALIGN(16) int32_t tensor_data3[4] = { 1, 1, 50, 8, };
const MODEL_SECTION(EI_MODEL_SECTION) ALIGN(16) int32_t tensor_data4[4] = { 1, 50, 1, 16, };
const MODEL_SECTION(EI_MODEL_SECTION) ALIGN(8) int32_t tensor_data5[2] = { -1, 400, };
const TfArray<1, int> tensor_dimension5 = { 1, { 2 } };
const MODEL_SECTION(EI_MODEL_SECTION) ALIGN(8) int32_t tensor_data6[3] = { 1578, -3268, 533, };
const TfArray<1, int> tensor_dimension6 = { 1, { 3 } };
const TfArray<1, float> quant6_scale = { 1, { 7.8392222349066287e-05, } };
const TfArray<1, int> quant6_zero = { 1, { 0 } };
const TfLiteAffineQuantization quant6 = { (TfLiteFloatArray*)&quant6_scale, (TfLiteIntArray*)&quant6_zero, 0 };
const MODEL_SECTION(EI_MODEL_SECTION) ALIGN(16) int8_t tensor_data7[3*400] = { 
  29, 11, 51, -7, 24, 28, 27, -4, -6, -5, 13, -4, 35, 1, 10, -3, -13, -3, 51, 23, -10, 12, 58, 23, -8, -19, 34, -6, 11, 1, 37, 6, 5, 13, 5, 14, -28, 6, 25, 22, 24, -12, 30, 26, 19, 5, 10, -41, -4, 11, 24, 2, -42, -38, -10, 2, 18, -74, -13, -75, -25, -37, -32, -53, -34, -68, -60, -12, 29, -37, -19, -17, -63, -33, -49, -52, 5, 20, 26, -25, -41, -56, 8, 9, 22, -50, 4, 14, -50, -18, -11, -32, 6, 25, 23, -30, 0, -15, 42, 35, 13, 23, 20, 37, 12, -15, -4, -14, 13, 42, 13, 32, 3, 31, 28, 10, 11, -9, -10, 27, 19, -22, 12, -23, 11, 12, 31, -9, 6, -9, -3, 31, 35, -30, 23, 13, 9, -21, -9, -15, 4, 16, 23, -2, -29, -6, -40, 17, 47, -20, 13, 8, -15, -47, -21, -29, 30, 15, 26, 17, 2, -27, -24, 31, -5, -28, -24, 11, -19, -48, -36, -33, 5, 9, 27, 23, -35, -35, -51, 2, 97, -61, -52, -5, -50, -42, -63, -83, -27, 5, 22, 13, -53, -39, -103, 5, 96, -85, -56, 46, -91, -48, -65, -63, 25, 48, 69, 46, -65, -52, -81, 5, 75, -95, -75, 23, -51, -39, -45, -39, 7, 38, 43, -8, -58, -27, -39, 10, 44, -42, -38, 9, -39, -11, -36, -21, 22, 46, 20, 11, -20, -12, -40, 13, 21, -49, -31, 35, -22, -9, -23, 6, 39, 36, 20, 6, -13, 4, 25, 27, 11, -10, -4, 21, -5, 7, 0, 3, 13, 20, 21, 24, -6, -20, 3, -1, 9, -19, -2, 2, -17, -27, -27, -25, -4, 15, 2, -9, -32, 5, 4, -11, 5, -19, 6, 27, -18, -32, -18, -17, 9, 40, 41, 27, -25, -8, 10, 8, 2, -10, 9, 32, -21, -7, -19, -24, 16, 22, 17, -24, 4, -29, 4, 10, 16, -18, -3, 37, -19, -38, -15, -26, 26, 26, 16, 20, -27, -3, -21, 18, 4, -11, 5, 23, -1, -21, -18, -12, 29, 15, 10, -2, -49, -20, -19, -10, 20, -9, 13, 16, -23, -35, 7, -4, 6, 10, 3, 11, -18, -4, 41, 26, 23, 22, 7, 8, 15, 14, 14, 17, -14, 8, -5, 18, -7, 17, 38, 17, -13, 25, 23, -12, 16, 20, 23, 9, -1, -12, -8, 11, 
  -47, -45, -126, -45, -27, -102, -87, -36, -111, -59, -127, -22, -11, -31, -20, -20, -15, -10, -99, -21, -15, -102, -76, -21, -74, -35, -83, 1, -17, -35, -17, -49, -11, -26, -72, -36, -21, -88, -94, -14, -67, -36, -101, -32, -38, -33, -36, -40, -9, -12, -70, -25, -11, -9, -23, -20, -37, 31, -11, 38, 27, 28, -5, -28, 30, 70, 34, 11, -20, 25, 38, 0, 22, 21, 15, 53, -20, -27, -33, 17, 22, 38, -27, -35, 19, 23, -14, -9, 29, 13, -46, 12, -9, -8, -14, 25, -40, -30, -66, -5, -33, -75, -18, -7, -83, -44, -93, -26, -21, -19, -9, 31, -40, -32, -49, 25, -9, -39, -18, -13, -34, -25, -61, -60, -31, -37, -30, -38, -41, -22, -85, -28, 5, -59, -105, -18, -60, -60, -114, -34, -6, -49, -2, -32, -42, -27, 1, -39, 20, -62, -46, 6, -59, -8, -41, 3, 14, 21, 32, 69, 9, 8, -9, -20, 27, 6, 32, 18, -13, 14, 5, 24, 37, 37, 40, 87, 3, -27, 11, 21, -90, 22, 16, 2, 11, 8, 11, -4, -20, -24, -24, 7, 4, -19, -8, -10, 24, -8, -3, 3, 0, -1, 6, 5, -18, -4, 2, -33, 10, -6, -1, -12, -7, 4, -12, -3, 6, 10, -6, 6, -3, -4, -14, 6, -12, 12, 8, 4, -4, -1, 11, -14, 18, -5, 16, 6, 15, -29, 8, -38, -5, 9, 6, 9, 20, -8, 10, 4, 0, 5, 17, 31, 13, -28, -3, -34, 8, 12, 7, -9, 21, 4, 3, 4, 12, 14, -8, 18, -9, -42, 2, -4, 2, 31, 0, 10, 47, 4, -1, -16, 8, 14, 16, 23, -1, 3, -7, 0, 19, 17, 9, -6, -25, 5, -11, -3, 14, -10, 8, 1, -21, -74, -45, 30, 16, -6, -10, -18, 63, 0, -1, -25, 18, 10, 2, 16, -3, -12, -8, 22, 16, 18, 0, 28, 37, 8, 8, -3, -3, 37, -14, 8, -3, -30, 17, -20, 22, 10, 11, -23, -56, 6, -9, 10, 1, 21, 13, 8, -7, -29, 51, -58, 7, 19, 30, -2, -50, 29, 2, -3, 9, 11, -13, -3, -8, -55, 2, -31, -1, 27, 2, 3, -124, 0, -32, -26, 17, 7, -17, 4, -20, -28, 2, -61, -14, 11, -7, -9, -90, -5, -1, -35, 6, -9, 6, -18, 8, -63, -3, -86, 
  38, 45, 113, 23, -31, 66, 62, -1, 108, 62, 117, 12, -18, 24, 1, 30, 34, 16, 55, 23, 19, 88, 42, -19, 37, 47, 66, 17, 7, 3, -4, 22, 8, 25, 64, 14, 39, 46, 59, 14, 38, 10, 45, 20, 23, 1, 10, 43, 15, -10, 34, 5, 34, 16, 27, 15, 21, 17, 27, -6, -7, -17, -11, 44, 29, -18, 4, 10, 0, -4, -7, 6, -3, 2, -7, 4, 24, 23, -16, -31, -20, -16, 55, 8, -34, 2, 4, 5, 15, 22, 18, 9, 1, 5, -19, -13, 16, 22, 38, 11, -2, 47, -12, -25, 64, 20, 34, 20, -4, -26, -18, -17, 17, -5, 45, -52, -7, 55, 40, 2, 32, 22, 37, 24, 5, 11, -10, -2, 35, 33, 58, 7, -28, 46, 82, -19, 25, 40, 74, 41, 30, 12, -14, 19, 34, 20, 39, 51, -40, 57, 62, -49, 41, 32, 45, 0, -50, -49, -61, -74, 5, 7, 12, 21, -34, 4, -10, -38, 13, -16, -14, -5, -38, -70, -57, -87, 21, 8, 5, -8, 10, -7, 13, 7, -7, 2, 2, 10, 26, 21, 16, 10, 15, 20, 19, 3, -91, 19, 20, 13, 14, 8, 7, 14, 9, -37, -72, 5, 2, -5, 9, 4, -73, 11, 9, -26, 3, 8, 1, 8, 16, -17, -37, 5, 1, 9, 6, 0, -68, 6, 1, -17, 22, -2, 12, 25, -15, -16, -50, 27, 4, 20, -6, 1, -33, 14, 8, -19, 9, -3, 10, -2, -8, -17, -43, 24, -4, -12, -20, 5, -49, -4, -15, 5, -6, 11, -12, 4, -11, -16, -31, -15, -3, -11, -1, -24, -58, -1, -12, -28, -8, 3, -1, -22, -2, -28, -28, -1, -14, -12, -10, -14, -24, -24, -6, 2, -11, -11, 9, -26, 5, 45, -11, -21, 1, 11, -14, 2, -88, 2, 2, 11, 3, -9, -11, -9, -25, -17, -18, 8, -11, 14, -13, -24, -46, -7, -4, -6, 17, 1, 1, -21, -6, -19, -34, 23, -4, -20, 4, 8, 31, -8, 2, 4, -9, -28, 1, 7, -8, -7, -36, 64, 3, -1, 1, -6, 49, 2, 10, -15, -24, -14, 10, -4, 23, 40, -23, 22, 9, -16, 16, -16, 83, 2, 16, 24, 1, -4, -5, 16, 2, 31, -1, 57, -5, 1, -18, -3, 72, -30, -14, 2, -20, 12, -18, 3, 16, 31, 20, 64, 
};
const TfArray<2, int> tensor_dimension7 = { 2, { 3,400 } };
const TfArray<1, float> quant7_scale = { 1, { 0.0069588329643011093, } };
const TfLiteAffineQuantization quant7 = { (TfLiteFloatArray*)&quant7_scale, (TfLiteIntArray*)&g0::quant6_zero, 0 };
const MODEL_SECTION(EI_MODEL_SECTION) ALIGN(16) int32_t tensor_data8[16] = { -6104, -2671, 2595, 3759, 5289, 828, 3091, 912, 695, -3677, 3111, -2421, 1582, 1376, 1338, 2009, };
const TfArray<1, int> tensor_dimension8 = { 1, { 16 } };
const TfArray<16, float> quant8_scale = { 16, { 3.1822099117562175e-05, 5.3408446547109634e-05, 5.7881909015122801e-05, 4.0801598515827209e-05, 6.5260901465080678e-05, 7.6918142440263182e-05, 6.2756793340668082e-05, 7.3493822128511965e-05, 9.2785194283351302e-05, 5.4842901590745896e-05, 6.6848988353740424e-05, 0.00010185991413891315, 5.89281007705722e-05, 0.00010359104635426775, 7.3883507866412401e-05, 0.0001198002792079933, } };
const TfArray<16, int> quant8_zero = { 16, { 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 } };
const TfLiteAffineQuantization quant8 = { (TfLiteFloatArray*)&quant8_scale, (TfLiteIntArray*)&quant8_zero, 0 };
const MODEL_SECTION(EI_MODEL_SECTION) ALIGN(16) int8_t tensor_data9[16*1*3*8] = { 
  /* [0][0][][] */ 53,31,127,118,-112,9,-88,71, -38,18,12,-26,-74,66,-10,-127, 37,-2,66,-16,-30,-24,-2,-62, 
  /* [1][0][][] */ -61,91,8,53,-11,-80,-18,-3, -51,-11,-33,-28,19,73,9,88, 12,-28,11,-127,0,-49,-9,-100, 
  /* [2][0][][] */ -39,-75,-20,16,-57,-111,-81,24, 93,-46,54,65,-92,-101,-98,79, -82,-1,17,-45,-127,33,-38,-17, 
  /* [3][0][][] */ 127,-80,51,-80,63,-110,-119,-5, 42,-124,-59,44,3,-54,-97,-21, -91,-54,-93,-81,108,21,52,-27, 
  /* [4][0][][] */ 41,36,21,-69,-68,-89,48,-12, 23,76,84,-78,-100,-127,121,-41, 98,86,125,-106,-103,-93,45,-39, 
  /* [5][0][][] */ 2,-48,42,34,-127,-68,-67,74, -35,-12,-10,-9,-29,42,-91,4, -38,-1,19,-10,-28,36,-108,-2, 
  /* [6][0][][] */ 94,-18,14,23,-105,-30,-57,-3, 66,-10,12,-60,-127,-28,-82,43, 14,-90,-88,77,-120,-16,-125,-80, 
  /* [7][0][][] */ 28,127,36,-67,-69,29,55,-16, 77,56,92,8,15,-37,44,0, 25,-15,-3,-42,25,-92,72,-4, 
  /* [8][0][][] */ 49,-127,17,39,-45,-19,-28,55, -8,-46,-36,20,3,-20,-104,10, -3,-22,36,-3,-4,39,-54,-24, 
  /* [9][0][][] */ 25,10,88,58,14,127,62,28, -35,44,25,-54,9,-37,-16,-1, -27,-97,-11,59,-16,47,-63,38, 
  /* [10][0][][] */ 8,-89,-53,32,-52,-24,-34,3, -19,-43,-25,-43,-98,-58,-101,28, -16,-18,-47,6,-123,32,-127,52, 
  /* [11][0][][] */ -73,55,3,31,127,96,25,-46, -41,72,24,-7,34,-59,4,21, -43,-61,-25,41,-7,-49,-28,4, 
  /* [12][0][][] */ -65,25,-21,5,-1,101,70,-63, 98,88,-12,-69,-92,-41,-55,-102, 76,27,72,21,5,-127,-58,50, 
  /* [13][0][][] */ 18,7,11,-42,-35,82,-13,-22, 84,73,15,-32,-52,-127,1,-43, 13,-23,79,12,14,-70,12,41, 
  /* [14][0][][] */ 25,13,73,-58,44,127,44,-97, 67,98,118,-36,18,-70,89,-77, 81,-86,74,33,23,-83,55,-42, 
  /* [15][0][][] */ 59,78,22,-38,-31,-40,-6,22, 30,22,109,-28,-62,-89,47,8, 12,-10,114,-36,-127,-61,81,14, 
};
const TfArray<4, int> tensor_dimension9 = { 4, { 16,1,3,8 } };
const TfArray<16, float> quant9_scale = { 16, { 0.0020799406338483095, 0.0034908568486571312, 0.0037832492962479591, 0.0026668542996048927, 0.0042655514553189278, 0.0050274860113859177, 0.0041018789634108543, 0.0048036673106253147, 0.0060645807534456253, 0.0035846149548888206, 0.0043693510815501213, 0.0066577182151377201, 0.0038516297936439514, 0.0067708673886954784, 0.0048291380517184734, 0.0078303273767232895, } };
const TfLiteAffineQuantization quant9 = { (TfLiteFloatArray*)&quant9_scale, (TfLiteIntArray*)&g0::quant8_zero, 0 };
const MODEL_SECTION(EI_MODEL_SECTION) ALIGN(16) int32_t tensor_data10[8] = { 4803, 2583, 93, -11565, 6398, 5102, 5196, -1892, };
const TfArray<1, int> tensor_dimension10 = { 1, { 8 } };
const TfArray<8, float> quant10_scale = { 8, { 2.3388209228869528e-05, 2.4697223125258461e-05, 2.932690222223755e-05, 2.0712355762952939e-05, 2.0848172425758094e-05, 2.4412589482381009e-05, 2.7342544854036532e-05, 1.9606581190600991e-05, } };
const TfArray<8, int> quant10_zero = { 8, { 0,0,0,0,0,0,0,0 } };
const TfLiteAffineQuantization quant10 = { (TfLiteFloatArray*)&quant10_scale, (TfLiteIntArray*)&quant10_zero, 0 };
const MODEL_SECTION(EI_MODEL_SECTION) ALIGN(16) int8_t tensor_data11[8*1*3*40] = { 
  /* [0][0][][] */ 79,-14,-59,-70,-74,-37,-14,-20,-22,-47,-11,-90,-4,-46,-66,-10,-40,-29,-15,-31,25,0,25,49,38,-21,8,-30,2,-38,-48,-10,-37,-65,-45,-69,-43,-51,-1,20, 81,7,-58,-35,-30,-39,54,20,5,-44,40,11,-24,67,41,49,39,17,7,59,-3,35,34,35,27,-7,42,18,49,-6,-21,17,-40,-68,-51,-27,-5,-9,-23,9, 127,13,-13,-87,6,-25,-31,-6,1,13,1,-44,3,-13,-11,1,-37,-14,36,11,43,1,17,42,37,-7,-1,33,20,-4,-24,-37,-33,-65,-49,-47,-19,23,22,20, 
  /* [1][0][][] */ 23,-44,-47,-41,-58,3,-48,24,2,-34,-16,-61,8,-54,16,-3,-75,-61,9,26,8,-22,47,51,47,-19,-40,-28,5,36,7,-47,-32,-18,-27,-34,-30,-42,-41,-11, 106,-19,-36,-25,-44,7,3,17,-10,-51,-35,-54,-14,-58,-71,8,-36,-74,-32,-15,19,31,14,35,0,11,10,-19,-7,42,-12,-28,-31,-18,-72,-75,-78,-54,-62,-70, 127,0,-1,27,19,77,74,29,-9,2,-8,40,28,-11,-3,9,-21,-5,16,-8,-8,13,-13,50,55,56,-8,27,-7,-15,13,-38,-18,-58,-21,26,20,-10,-48,-13, 
  /* [2][0][][] */ 116,29,-56,-45,-57,-67,-8,-12,-40,-35,9,-68,-14,-45,-28,-9,-7,-43,0,3,21,2,17,28,34,-23,-16,-21,3,-21,-25,0,-35,-11,-7,-50,-4,-35,-20,-19, 127,-8,-19,-54,-32,-32,-13,-38,4,-53,25,-74,-48,-2,6,-12,-55,-38,6,18,7,29,8,28,11,33,-10,4,-17,-19,-17,10,-44,-28,-45,-55,-50,-5,-38,-35, 125,-21,-51,-45,-14,-36,3,23,4,-31,-21,-39,-13,2,-9,-7,-47,-42,11,-27,22,17,6,33,52,-11,20,-10,1,22,9,-20,-21,-56,-15,-40,-22,2,-34,-30, 
  /* [3][0][][] */ 15,7,96,44,127,-6,49,53,66,-2,20,11,6,34,31,1,69,30,14,4,18,53,27,21,17,35,28,-27,35,6,18,31,28,25,29,30,9,15,-19,31, 16,12,104,33,51,22,91,-10,38,49,-15,12,-5,-24,24,-37,51,33,-15,5,21,22,-18,15,-5,-30,10,-27,-1,-27,21,35,-4,-21,-32,-18,-2,-25,34,-27, 30,-10,82,-41,37,14,20,8,-23,7,19,-9,-29,8,-31,-22,16,20,-25,18,-37,21,-41,-13,-29,-6,22,3,20,-49,20,-35,-17,-19,-40,-13,2,-47,-42,-36, 
  /* [4][0][][] */ 37,-42,-41,12,-42,-9,-23,2,-2,-16,-30,-11,62,32,-83,-18,-96,-51,-68,-62,-52,-109,-35,-17,-77,-4,5,48,84,11,-26,-4,-52,-20,-25,-82,-11,31,-18,-99, -36,-10,-6,-48,-37,-15,2,-42,20,-34,-13,1,31,33,1,-25,-79,-35,-8,-38,-68,-9,18,26,-75,-27,-28,-30,18,9,-65,20,-52,-45,14,-2,-70,-7,-92,-99, -57,-55,-45,-20,-35,-26,9,-2,-8,-43,-34,19,26,-17,-98,-32,-62,-57,-47,-21,-45,-60,-25,-21,-59,-104,-99,-71,-41,44,-52,-71,8,-2,-86,1,-15,-19,-47,-127, 
  /* [5][0][][] */ -70,-6,-7,7,20,5,-10,12,16,13,-13,8,-4,16,-29,-12,-47,-16,-18,3,-58,-75,-9,-71,-39,-45,-37,-13,-34,21,-52,-1,2,-10,28,14,10,-29,38,-63, -73,-56,22,25,10,-10,-23,-9,-17,-19,-60,-42,-4,11,-28,-12,-30,-34,-7,-11,-35,-84,-32,-52,-64,-25,-58,2,-11,-18,-37,-17,-30,3,-28,16,-5,29,-44,-80, 14,34,103,66,61,39,18,45,39,26,-14,-30,-39,0,-37,-2,22,-16,-5,-64,-5,21,-19,-7,-64,-36,23,49,42,7,3,20,-32,-89,-117,-127,-67,-83,-59,-62, 
  /* [6][0][][] */ 95,25,-71,-45,-55,-83,-74,-34,-69,-103,-48,-38,-77,5,-25,-39,-45,-16,-28,-9,-39,14,8,47,24,6,27,-4,32,25,33,-9,26,-29,-54,-25,-58,-3,-40,-1, 77,-38,-41,-53,-58,-24,-16,-13,-30,-48,-42,-40,-57,36,22,-4,-44,-45,17,28,-24,-41,17,33,3,46,-7,15,15,18,44,28,-17,-6,-56,-20,7,-11,-48,-20, 127,5,-49,-39,-48,-20,-11,1,23,-3,-22,-44,-14,-15,-18,-19,-10,-57,-13,-4,-13,-19,61,48,41,17,-34,-22,-6,25,-20,-16,-20,-2,-46,-30,-49,-56,-70,-15, 
  /* [7][0][][] */ 37,65,127,102,102,70,16,70,-4,25,62,9,29,-15,20,9,40,87,-14,-6,-7,37,-52,-39,28,-18,-9,38,28,12,-21,-6,27,46,10,-5,61,24,57,-1, -8,6,68,3,3,-51,29,52,14,53,56,-6,30,13,-22,-36,-7,6,-6,-33,-9,12,-14,-20,-9,-27,-29,32,17,-37,-42,9,27,12,27,16,38,40,32,29, -34,-18,-10,-27,-59,-27,-5,-20,-3,20,-30,13,-39,10,6,1,27,52,6,-19,-14,17,-14,-15,-50,14,0,27,-17,0,2,31,-40,-24,-47,-4,-24,-22,32,8, 
};
const TfArray<4, int> tensor_dimension11 = { 4, { 8,1,3,40 } };
const TfArray<8, float> quant11_scale = { 8, { 0.0060347127728164196, 0.0063724694773554802, 0.0075670364312827587, 0.0053442791104316711, 0.0053793229162693024, 0.0062990272417664528, 0.0070550250820815563, 0.0050589628517627716, } };
const TfLiteAffineQuantization quant11 = { (TfLiteFloatArray*)&quant11_scale, (TfLiteIntArray*)&g0::quant10_zero, 0 };
const TfArray<4, int> tensor_dimension12 = { 4, { 1,1,99,40 } };
const TfArray<4, int> tensor_dimension13 = { 4, { 1,1,99,8 } };
const TfArray<1, float> quant13_scale = { 1, { 0.015299523249268532, } };
const TfLiteAffineQuantization quant13 = { (TfLiteFloatArray*)&quant13_scale, (TfLiteIntArray*)&g0::quant0_zero, 0 };
const TfArray<4, int> tensor_dimension14 = { 4, { 1,99,1,8 } };
const TfArray<4, int> tensor_dimension15 = { 4, { 1,50,1,8 } };
const TfArray<4, int> tensor_dimension16 = { 4, { 1,1,50,8 } };
const TfArray<4, int> tensor_dimension17 = { 4, { 1,1,50,16 } };
const TfArray<1, float> quant17_scale = { 1, { 0.01126513909548521, } };
const TfLiteAffineQuantization quant17 = { (TfLiteFloatArray*)&quant17_scale, (TfLiteIntArray*)&g0::quant0_zero, 0 };
const TfArray<4, int> tensor_dimension18 = { 4, { 1,50,1,16 } };
const TfArray<4, int> tensor_dimension19 = { 4, { 1,25,1,16 } };
const TfArray<2, int> tensor_dimension20 = { 2, { 1,400 } };
const TfArray<2, int> tensor_dimension21 = { 2, { 1,3 } };
const TfArray<1, float> quant21_scale = { 1, { 0.15572158992290497, } };
const TfArray<1, int> quant21_zero = { 1, { 18 } };
const TfLiteAffineQuantization quant21 = { (TfLiteFloatArray*)&quant21_scale, (TfLiteIntArray*)&quant21_zero, 0 };
const TfArray<1, float> quant22_scale = { 1, { 0.00390625, } };
const TfLiteAffineQuantization quant22 = { (TfLiteFloatArray*)&quant22_scale, (TfLiteIntArray*)&g0::quant0_zero, 0 };
const TfLiteReshapeParams opdata0 = { { 0, 0, 0, 0, 0, 0, 0, 0, }, 0 };
const TfArray<2, int> inputs0 = { 2, { 0,1 } };
const TfArray<1, int> outputs0 = { 1, { 12 } };
const TfLiteConvParams opdata1 = { kTfLitePaddingSame, 1,1, kTfLiteActRelu, 1,1 };
const TfArray<3, int> inputs1 = { 3, { 12,11,10 } };
const TfArray<1, int> outputs1 = { 1, { 13 } };
const TfLiteReshapeParams opdata2 = { { 0, 0, 0, 0, 0, 0, 0, 0, }, 0 };
const TfArray<2, int> inputs2 = { 2, { 13,2 } };
const TfArray<1, int> outputs2 = { 1, { 14 } };
const TfLitePoolParams opdata3 = { kTfLitePaddingSame, 1,2, 1,2, kTfLiteActNone, { { 0,0, 0, 0 } } };
const TfArray<1, int> inputs3 = { 1, { 14 } };
const TfArray<1, int> outputs3 = { 1, { 15 } };
const TfLiteReshapeParams opdata4 = { { 0, 0, 0, 0, 0, 0, 0, 0, }, 0 };
const TfArray<2, int> inputs4 = { 2, { 15,3 } };
const TfArray<1, int> outputs4 = { 1, { 16 } };
const TfLiteConvParams opdata5 = { kTfLitePaddingSame, 1,1, kTfLiteActRelu, 1,1 };
const TfArray<3, int> inputs5 = { 3, { 16,9,8 } };
const TfArray<1, int> outputs5 = { 1, { 17 } };
const TfLiteReshapeParams opdata6 = { { 0, 0, 0, 0, 0, 0, 0, 0, }, 0 };
const TfArray<2, int> inputs6 = { 2, { 17,4 } };
const TfArray<1, int> outputs6 = { 1, { 18 } };
const TfLitePoolParams opdata7 = { kTfLitePaddingSame, 1,2, 1,2, kTfLiteActNone, { { 0,0, 0, 0 } } };
const TfArray<1, int> inputs7 = { 1, { 18 } };
const TfArray<1, int> outputs7 = { 1, { 19 } };
const TfLiteReshapeParams opdata8 = { { 0, 0, 0, 0, 0, 0, 0, 0, }, 0 };
const TfArray<2, int> inputs8 = { 2, { 19,5 } };
const TfArray<1, int> outputs8 = { 1, { 20 } };
const TfLiteFullyConnectedParams opdata9 = { kTfLiteActNone, kTfLiteFullyConnectedWeightsFormatDefault, false, false };
const TfArray<3, int> inputs9 = { 3, { 20,7,6 } };
const TfArray<1, int> outputs9 = { 1, { 21 } };
const TfLiteSoftmaxParams opdata10 = { 1 };
const TfArray<1, int> inputs10 = { 1, { 21 } };
const TfArray<1, int> outputs10 = { 1, { 22 } };
};

TensorInfo_t tensorData[] = {
{ kTfLiteArenaRw, kTfLiteInt8, (int32_t*)(tensor_arena + 3968), (TfLiteIntArray*)&g0::tensor_dimension0, 3960, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&g0::quant0))}, },
{ kTfLiteMmapRo, kTfLiteInt32, (int32_t*)g0::tensor_data1, (TfLiteIntArray*)&g0::tensor_dimension1, 16, {kTfLiteNoQuantization, nullptr}, },
{ kTfLiteMmapRo, kTfLiteInt32, (int32_t*)g0::tensor_data2, (TfLiteIntArray*)&g0::tensor_dimension1, 16, {kTfLiteNoQuantization, nullptr}, },
{ kTfLiteMmapRo, kTfLiteInt32, (int32_t*)g0::tensor_data3, (TfLiteIntArray*)&g0::tensor_dimension1, 16, {kTfLiteNoQuantization, nullptr}, },
{ kTfLiteMmapRo, kTfLiteInt32, (int32_t*)g0::tensor_data4, (TfLiteIntArray*)&g0::tensor_dimension1, 16, {kTfLiteNoQuantization, nullptr}, },
{ kTfLiteMmapRo, kTfLiteInt32, (int32_t*)g0::tensor_data5, (TfLiteIntArray*)&g0::tensor_dimension5, 8, {kTfLiteNoQuantization, nullptr}, },
{ kTfLiteMmapRo, kTfLiteInt32, (int32_t*)g0::tensor_data6, (TfLiteIntArray*)&g0::tensor_dimension6, 12, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&g0::quant6))}, },
{ kTfLiteMmapRo, kTfLiteInt8, (int32_t*)g0::tensor_data7, (TfLiteIntArray*)&g0::tensor_dimension7, 1200, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&g0::quant7))}, },
{ kTfLiteMmapRo, kTfLiteInt32, (int32_t*)g0::tensor_data8, (TfLiteIntArray*)&g0::tensor_dimension8, 64, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&g0::quant8))}, },
{ kTfLiteMmapRo, kTfLiteInt8, (int32_t*)g0::tensor_data9, (TfLiteIntArray*)&g0::tensor_dimension9, 384, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&g0::quant9))}, },
{ kTfLiteMmapRo, kTfLiteInt32, (int32_t*)g0::tensor_data10, (TfLiteIntArray*)&g0::tensor_dimension10, 32, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&g0::quant10))}, },
{ kTfLiteMmapRo, kTfLiteInt8, (int32_t*)g0::tensor_data11, (TfLiteIntArray*)&g0::tensor_dimension11, 960, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&g0::quant11))}, },
{ kTfLiteArenaRw, kTfLiteInt8, (int32_t*)(tensor_arena + 0), (TfLiteIntArray*)&g0::tensor_dimension12, 3960, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&g0::quant0))}, },
{ kTfLiteArenaRw, kTfLiteInt8, (int32_t*)(tensor_arena + 3968), (TfLiteIntArray*)&g0::tensor_dimension13, 792, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&g0::quant13))}, },
{ kTfLiteArenaRw, kTfLiteInt8, (int32_t*)(tensor_arena + 0), (TfLiteIntArray*)&g0::tensor_dimension14, 792, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&g0::quant13))}, },
{ kTfLiteArenaRw, kTfLiteInt8, (int32_t*)(tensor_arena + 800), (TfLiteIntArray*)&g0::tensor_dimension15, 400, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&g0::quant13))}, },
{ kTfLiteArenaRw, kTfLiteInt8, (int32_t*)(tensor_arena + 0), (TfLiteIntArray*)&g0::tensor_dimension16, 400, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&g0::quant13))}, },
{ kTfLiteArenaRw, kTfLiteInt8, (int32_t*)(tensor_arena + 800), (TfLiteIntArray*)&g0::tensor_dimension17, 800, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&g0::quant17))}, },
{ kTfLiteArenaRw, kTfLiteInt8, (int32_t*)(tensor_arena + 0), (TfLiteIntArray*)&g0::tensor_dimension18, 800, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&g0::quant17))}, },
{ kTfLiteArenaRw, kTfLiteInt8, (int32_t*)(tensor_arena + 800), (TfLiteIntArray*)&g0::tensor_dimension19, 400, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&g0::quant17))}, },
{ kTfLiteArenaRw, kTfLiteInt8, (int32_t*)(tensor_arena + 0), (TfLiteIntArray*)&g0::tensor_dimension20, 400, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&g0::quant17))}, },
{ kTfLiteArenaRw, kTfLiteInt8, (int32_t*)(tensor_arena + 400), (TfLiteIntArray*)&g0::tensor_dimension21, 3, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&g0::quant21))}, },
{ kTfLiteArenaRw, kTfLiteInt8, (int32_t*)(tensor_arena + 0), (TfLiteIntArray*)&g0::tensor_dimension21, 3, {kTfLiteAffineQuantization, const_cast<void*>(static_cast<const void*>(&g0::quant22))}, },
};

#ifndef TF_LITE_STATIC_MEMORY
TfLiteNode tflNodes[11] = {
{ (TfLiteIntArray*)&g0::inputs0, (TfLiteIntArray*)&g0::outputs0, (TfLiteIntArray*)&g0::inputs0, nullptr, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata0)), nullptr, 0, },
{ (TfLiteIntArray*)&g0::inputs1, (TfLiteIntArray*)&g0::outputs1, (TfLiteIntArray*)&g0::inputs1, nullptr, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata1)), nullptr, 0, },
{ (TfLiteIntArray*)&g0::inputs2, (TfLiteIntArray*)&g0::outputs2, (TfLiteIntArray*)&g0::inputs2, nullptr, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata2)), nullptr, 0, },
{ (TfLiteIntArray*)&g0::inputs3, (TfLiteIntArray*)&g0::outputs3, (TfLiteIntArray*)&g0::inputs3, nullptr, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata3)), nullptr, 0, },
{ (TfLiteIntArray*)&g0::inputs4, (TfLiteIntArray*)&g0::outputs4, (TfLiteIntArray*)&g0::inputs4, nullptr, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata4)), nullptr, 0, },
{ (TfLiteIntArray*)&g0::inputs5, (TfLiteIntArray*)&g0::outputs5, (TfLiteIntArray*)&g0::inputs5, nullptr, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata5)), nullptr, 0, },
{ (TfLiteIntArray*)&g0::inputs6, (TfLiteIntArray*)&g0::outputs6, (TfLiteIntArray*)&g0::inputs6, nullptr, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata6)), nullptr, 0, },
{ (TfLiteIntArray*)&g0::inputs7, (TfLiteIntArray*)&g0::outputs7, (TfLiteIntArray*)&g0::inputs7, nullptr, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata7)), nullptr, 0, },
{ (TfLiteIntArray*)&g0::inputs8, (TfLiteIntArray*)&g0::outputs8, (TfLiteIntArray*)&g0::inputs8, nullptr, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata8)), nullptr, 0, },
{ (TfLiteIntArray*)&g0::inputs9, (TfLiteIntArray*)&g0::outputs9, (TfLiteIntArray*)&g0::inputs9, nullptr, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata9)), nullptr, 0, },
{ (TfLiteIntArray*)&g0::inputs10, (TfLiteIntArray*)&g0::outputs10, (TfLiteIntArray*)&g0::inputs10, nullptr, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata10)), nullptr, 0, },
};
#else
TfLiteNode tflNodes[11] = {
{ (TfLiteIntArray*)&g0::inputs0, (TfLiteIntArray*)&g0::outputs0, (TfLiteIntArray*)&g0::inputs0, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata0)), nullptr, 0, },
{ (TfLiteIntArray*)&g0::inputs1, (TfLiteIntArray*)&g0::outputs1, (TfLiteIntArray*)&g0::inputs1, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata1)), nullptr, 0, },
{ (TfLiteIntArray*)&g0::inputs2, (TfLiteIntArray*)&g0::outputs2, (TfLiteIntArray*)&g0::inputs2, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata2)), nullptr, 0, },
{ (TfLiteIntArray*)&g0::inputs3, (TfLiteIntArray*)&g0::outputs3, (TfLiteIntArray*)&g0::inputs3, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata3)), nullptr, 0, },
{ (TfLiteIntArray*)&g0::inputs4, (TfLiteIntArray*)&g0::outputs4, (TfLiteIntArray*)&g0::inputs4, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata4)), nullptr, 0, },
{ (TfLiteIntArray*)&g0::inputs5, (TfLiteIntArray*)&g0::outputs5, (TfLiteIntArray*)&g0::inputs5, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata5)), nullptr, 0, },
{ (TfLiteIntArray*)&g0::inputs6, (TfLiteIntArray*)&g0::outputs6, (TfLiteIntArray*)&g0::inputs6, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata6)), nullptr, 0, },
{ (TfLiteIntArray*)&g0::inputs7, (TfLiteIntArray*)&g0::outputs7, (TfLiteIntArray*)&g0::inputs7, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata7)), nullptr, 0, },
{ (TfLiteIntArray*)&g0::inputs8, (TfLiteIntArray*)&g0::outputs8, (TfLiteIntArray*)&g0::inputs8, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata8)), nullptr, 0, },
{ (TfLiteIntArray*)&g0::inputs9, (TfLiteIntArray*)&g0::outputs9, (TfLiteIntArray*)&g0::inputs9, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata9)), nullptr, 0, },
{ (TfLiteIntArray*)&g0::inputs10, (TfLiteIntArray*)&g0::outputs10, (TfLiteIntArray*)&g0::inputs10, nullptr, const_cast<void*>(static_cast<const void*>(&g0::opdata10)), nullptr, 0, },
};
#endif

used_operators_e used_ops[] =
{OP_RESHAPE, OP_CONV_2D, OP_RESHAPE, OP_MAX_POOL_2D, OP_RESHAPE, OP_CONV_2D, OP_RESHAPE, OP_MAX_POOL_2D, OP_RESHAPE, OP_FULLY_CONNECTED, OP_SOFTMAX, };


// Indices into tflTensors and tflNodes for subgraphs
const size_t tflTensors_subgraph_index[] = {0, 23, };
const size_t tflNodes_subgraph_index[] = {0, 11, };

// Input/output tensors
static const int in_tensor_indices[] = {
  0, 
};

static const int out_tensor_indices[] = {
  22, 
};


size_t current_subgraph_index = 0;

static void init_tflite_tensor(size_t i, TfLiteTensor *tensor) {
  tensor->type = tensorData[i].type;
  tensor->is_variable = false;

#if defined(EI_CLASSIFIER_ALLOCATION_HEAP)
  tensor->allocation_type = tensorData[i].allocation_type;
#else
  tensor->allocation_type = (tensor_arena <= tensorData[i].data && tensorData[i].data < tensor_arena + kTensorArenaSize) ? kTfLiteArenaRw : kTfLiteMmapRo;
#endif
  tensor->bytes = tensorData[i].bytes;
  tensor->dims = tensorData[i].dims;

#if defined(EI_CLASSIFIER_ALLOCATION_HEAP)
  if(tensor->allocation_type == kTfLiteArenaRw){
    uint8_t* start = (uint8_t*) ((uintptr_t)tensorData[i].data + (uintptr_t) tensor_arena);

    tensor->data.data =  start;
  }
  else {
      tensor->data.data = tensorData[i].data;
  }
#else
  tensor->data.data = tensorData[i].data;
#endif // EI_CLASSIFIER_ALLOCATION_HEAP
  tensor->quantization = tensorData[i].quantization;
  if (tensor->quantization.type == kTfLiteAffineQuantization) {
    TfLiteAffineQuantization const* quant = ((TfLiteAffineQuantization const*)(tensorData[i].quantization.params));
    tensor->params.scale = quant->scale->data[0];
    tensor->params.zero_point = quant->zero_point->data[0];
  }

}

static void init_tflite_eval_tensor(int i, TfLiteEvalTensor *tensor) {

  tensor->type = tensorData[i].type;

  tensor->dims = tensorData[i].dims;

#if defined(EI_CLASSIFIER_ALLOCATION_HEAP)
  auto allocation_type = tensorData[i].allocation_type;
  if(allocation_type == kTfLiteArenaRw) {
    uint8_t* start = (uint8_t*) ((uintptr_t)tensorData[i].data + (uintptr_t) tensor_arena);

    tensor->data.data =  start;
  }
  else {
    tensor->data.data = tensorData[i].data;
  }
#else
  tensor->data.data = tensorData[i].data;
#endif // EI_CLASSIFIER_ALLOCATION_HEAP
}

static void* overflow_buffers[EI_MAX_OVERFLOW_BUFFER_COUNT];
static size_t overflow_buffers_ix = 0;
static void * AllocatePersistentBufferImpl(struct TfLiteContext* ctx,
                                       size_t bytes) {
  void *ptr;
  uint32_t align_bytes = (bytes % 16) ? 16 - (bytes % 16) : 0;

  if (current_location - (bytes + align_bytes) < tensor_boundary) {
    if (overflow_buffers_ix > EI_MAX_OVERFLOW_BUFFER_COUNT - 1) {
      ei_printf("ERR: Failed to allocate persistent buffer of size %d, does not fit in tensor arena and reached EI_MAX_OVERFLOW_BUFFER_COUNT\n",
        (int)bytes);
      return NULL;
    }

    // OK, this will look super weird, but.... we have CMSIS-NN buffers which
    // we cannot calculate beforehand easily.
    ptr = ei_calloc(bytes, 1);
    if (ptr == NULL) {
      ei_printf("ERR: Failed to allocate persistent buffer of size %d\n", (int)bytes);
      return NULL;
    }
    overflow_buffers[overflow_buffers_ix++] = ptr;
    return ptr;
  }

  current_location -= bytes;

  // align to the left aligned boundary of 16 bytes
  current_location -= 15; // for alignment
  current_location += 16 - ((uintptr_t)(current_location) & 15);

  ptr = current_location;
  memset(ptr, 0, bytes);

  return ptr;
}

typedef struct {
  size_t bytes;
  void *ptr;
} scratch_buffer_t;

static scratch_buffer_t scratch_buffers[EI_MAX_SCRATCH_BUFFER_COUNT];
static size_t scratch_buffers_ix = 0;

static TfLiteStatus RequestScratchBufferInArenaImpl(struct TfLiteContext* ctx, size_t bytes,
                                                int* buffer_idx) {
  if (scratch_buffers_ix > EI_MAX_SCRATCH_BUFFER_COUNT - 1) {
    ei_printf("ERR: Failed to allocate scratch buffer of size %d, reached EI_MAX_SCRATCH_BUFFER_COUNT\n",
      (int)bytes);
    return kTfLiteError;
  }

  scratch_buffer_t b;
  b.bytes = bytes;

  b.ptr = AllocatePersistentBufferImpl(ctx, b.bytes);
  if (!b.ptr) {
    ei_printf("ERR: Failed to allocate scratch buffer of size %d\n",
      (int)bytes);
    return kTfLiteError;
  }

  scratch_buffers[scratch_buffers_ix] = b;
  *buffer_idx = scratch_buffers_ix;

  scratch_buffers_ix++;

  return kTfLiteOk;
}

static void* GetScratchBufferImpl(struct TfLiteContext* ctx, int buffer_idx) {
  if (buffer_idx > (int)scratch_buffers_ix) {
    return NULL;
  }
  return scratch_buffers[buffer_idx].ptr;
}

static const uint16_t TENSOR_IX_UNUSED = 0x7FFF;

static void ResetTensors() {
  for (size_t ix = 0; ix < MAX_TFL_TENSOR_COUNT; ix++) {
    tflTensors[ix].index = TENSOR_IX_UNUSED;
  }
  for (size_t ix = 0; ix < MAX_TFL_EVAL_COUNT; ix++) {
    tflEvalTensors[ix].index = TENSOR_IX_UNUSED;
  }
}

static TfLiteTensor* GetTensorImpl(const struct TfLiteContext* context,
                               int tensor_idx) {

  tensor_idx = tflTensors_subgraph_index[current_subgraph_index] + tensor_idx;

  for (size_t ix = 0; ix < MAX_TFL_TENSOR_COUNT; ix++) {
    // already used? OK!
    if (tflTensors[ix].index == tensor_idx) {
      return &tflTensors[ix].tensor;
    }
    // passed all the ones we've used, so end of the list?
    if (tflTensors[ix].index == TENSOR_IX_UNUSED) {
      // init the tensor
      init_tflite_tensor(tensor_idx, &tflTensors[ix].tensor);
      tflTensors[ix].index = tensor_idx;
      return &tflTensors[ix].tensor;
    }
  }

  ei_printf("ERR: GetTensor called beyond MAX_TFL_TENSOR_COUNT (%d)\n", MAX_TFL_TENSOR_COUNT);
  return nullptr;
}

static TfLiteEvalTensor* GetEvalTensorImpl(const struct TfLiteContext* context,
                                       int tensor_idx) {

  tensor_idx = tflTensors_subgraph_index[current_subgraph_index] + tensor_idx;

  for (size_t ix = 0; ix < MAX_TFL_EVAL_COUNT; ix++) {
    // already used? OK!
    if (tflEvalTensors[ix].index == tensor_idx) {
      return &tflEvalTensors[ix].tensor;
    }
    // passed all the ones we've used, so end of the list?
    if (tflEvalTensors[ix].index == TENSOR_IX_UNUSED) {
      // init the tensor
      init_tflite_eval_tensor(tensor_idx, &tflEvalTensors[ix].tensor);
      tflEvalTensors[ix].index = tensor_idx;
      return &tflEvalTensors[ix].tensor;
    }
  }

  ei_printf("ERR: GetTensor called beyond MAX_TFL_EVAL_COUNT (%d)\n", (int)MAX_TFL_EVAL_COUNT);
  return nullptr;
}

class EonMicroContext : public MicroContext {
 public:
 
  EonMicroContext(): MicroContext(nullptr, nullptr, nullptr) { }

  void* AllocatePersistentBuffer(size_t bytes) {
    return AllocatePersistentBufferImpl(nullptr, bytes);
  }

  TfLiteStatus RequestScratchBufferInArena(size_t bytes,
                                           int* buffer_index) {
  return RequestScratchBufferInArenaImpl(nullptr, bytes, buffer_index);
  }

  void* GetScratchBuffer(int buffer_index) {
    return GetScratchBufferImpl(nullptr, buffer_index);
  }
 
  TfLiteTensor* AllocateTempTfLiteTensor(int tensor_index) {
    return GetTensorImpl(nullptr, tensor_index);
  }

  void DeallocateTempTfLiteTensor(TfLiteTensor* tensor) {
    return;
  }

  bool IsAllTempTfLiteTensorDeallocated() {
    return true;
  }

  TfLiteEvalTensor* GetEvalTensor(int tensor_index) {
    return GetEvalTensorImpl(nullptr, tensor_index);
  }

};


} // namespace

TfLiteStatus tflite_learn_842824_3_init( void*(*alloc_fnc)(size_t,size_t) ) {
#ifdef EI_CLASSIFIER_ALLOCATION_HEAP
  tensor_arena = (uint8_t*) alloc_fnc(16, kTensorArenaSize);
  if (!tensor_arena) {
    ei_printf("ERR: failed to allocate tensor arena\n");
    return kTfLiteError;
  }
#else
  memset(tensor_arena, 0, kTensorArenaSize);
#endif
  tensor_boundary = tensor_arena;
  current_location = tensor_arena + kTensorArenaSize;

  EonMicroContext micro_context_;
  
  // Set microcontext as the context ptr
  ctx.impl_ = static_cast<void*>(&micro_context_);
  // Setup tflitecontext functions
  ctx.AllocatePersistentBuffer = &AllocatePersistentBufferImpl;
  ctx.RequestScratchBufferInArena = &RequestScratchBufferInArenaImpl;
  ctx.GetScratchBuffer = &GetScratchBufferImpl;
  ctx.GetTensor = &GetTensorImpl;
  ctx.GetEvalTensor = &GetEvalTensorImpl;
  ctx.ReportError = &MicroContextReportOpError;

  ctx.tensors_size = 23;
  for (size_t i = 0; i < 23; ++i) {
    TfLiteTensor tensor;
    init_tflite_tensor(i, &tensor);
    if (tensor.allocation_type == kTfLiteArenaRw) {
      auto data_end_ptr = (uint8_t*)tensor.data.data + tensorData[i].bytes;
      if (data_end_ptr > tensor_boundary) {
        tensor_boundary = data_end_ptr;
      }
    }
  }

  if (tensor_boundary > current_location /* end of arena size */) {
    ei_printf("ERR: tensor arena is too small, does not fit model - even without scratch buffers\n");
    return kTfLiteError;
  }

  registrations[OP_RESHAPE] = Register_RESHAPE();
  registrations[OP_CONV_2D] = Register_CONV_2D();
  registrations[OP_MAX_POOL_2D] = Register_MAX_POOL_2D();
  registrations[OP_FULLY_CONNECTED] = Register_FULLY_CONNECTED();
  registrations[OP_SOFTMAX] = Register_SOFTMAX();

  for (size_t g = 0; g < 1; ++g) {
    current_subgraph_index = g;
    for(size_t i = tflNodes_subgraph_index[g]; i < tflNodes_subgraph_index[g+1]; ++i) {
      if (registrations[used_ops[i]].init) {
        tflNodes[i].user_data = registrations[used_ops[i]].init(&ctx, (const char*)tflNodes[i].builtin_data, 0);
      }
    }
  }
  current_subgraph_index = 0;

  for(size_t g = 0; g < 1; ++g) {
    current_subgraph_index = g;
    for(size_t i = tflNodes_subgraph_index[g]; i < tflNodes_subgraph_index[g+1]; ++i) {
      if (registrations[used_ops[i]].prepare) {
        ResetTensors();
        TfLiteStatus status = registrations[used_ops[i]].prepare(&ctx, &tflNodes[i]);
        if (status != kTfLiteOk) {
          return status;
        }
      }
    }
  }
  current_subgraph_index = 0;

  return kTfLiteOk;
}

TfLiteStatus tflite_learn_842824_3_input(int index, TfLiteTensor *tensor) {
  init_tflite_tensor(in_tensor_indices[index], tensor);
  return kTfLiteOk;
}

TfLiteStatus tflite_learn_842824_3_output(int index, TfLiteTensor *tensor) {
  init_tflite_tensor(out_tensor_indices[index], tensor);
  return kTfLiteOk;
}

TfLiteStatus tflite_learn_842824_3_invoke() {
  for (size_t i = 0; i < 11; ++i) {
    ResetTensors();

    TfLiteStatus status = registrations[used_ops[i]].invoke(&ctx, &tflNodes[i]);

#if EI_CLASSIFIER_PRINT_STATE
    ei_printf("layer %lu\n", i);
    ei_printf("    inputs:\n");
    for (size_t ix = 0; ix < tflNodes[i].inputs->size; ix++) {
      auto d = tensorData[tflNodes[i].inputs->data[ix]];

      size_t data_ptr = (size_t)d.data;

      if (d.allocation_type == kTfLiteArenaRw) {
        data_ptr = (size_t)tensor_arena + data_ptr;
      }

      if (d.type == TfLiteType::kTfLiteInt8) {
        int8_t* data = (int8_t*)data_ptr;
        ei_printf("        %lu (%zu bytes, ptr=%p, alloc_type=%d, type=%d): ", ix, d.bytes, data, (int)d.allocation_type, (int)d.type);
        for (size_t jx = 0; jx < d.bytes; jx++) {
          ei_printf("%d ", data[jx]);
        }
      }
      else {
        float* data = (float*)data_ptr;
        ei_printf("        %lu (%zu bytes, ptr=%p, alloc_type=%d, type=%d): ", ix, d.bytes, data, (int)d.allocation_type, (int)d.type);
        for (size_t jx = 0; jx < d.bytes / 4; jx++) {
          ei_printf("%f ", data[jx]);
        }
      }
      ei_printf("\n");
    }
    ei_printf("\n");

    ei_printf("    outputs:\n");
    for (size_t ix = 0; ix < tflNodes[i].outputs->size; ix++) {
      auto d = tensorData[tflNodes[i].outputs->data[ix]];

      size_t data_ptr = (size_t)d.data;

      if (d.allocation_type == kTfLiteArenaRw) {
        data_ptr = (size_t)tensor_arena + data_ptr;
      }

      if (d.type == TfLiteType::kTfLiteInt8) {
        int8_t* data = (int8_t*)data_ptr;
        ei_printf("        %lu (%zu bytes, ptr=%p, alloc_type=%d, type=%d): ", ix, d.bytes, data, (int)d.allocation_type, (int)d.type);
        for (size_t jx = 0; jx < d.bytes; jx++) {
          ei_printf("%d ", data[jx]);
        }
      }
      else {
        float* data = (float*)data_ptr;
        ei_printf("        %lu (%zu bytes, ptr=%p, alloc_type=%d, type=%d): ", ix, d.bytes, data, (int)d.allocation_type, (int)d.type);
        for (size_t jx = 0; jx < d.bytes / 4; jx++) {
          ei_printf("%f ", data[jx]);
        }
      }
      ei_printf("\n");
    }
    ei_printf("\n");
#endif // EI_CLASSIFIER_PRINT_STATE

    if (status != kTfLiteOk) {
      return status;
    }
  }
  return kTfLiteOk;
}

TfLiteStatus tflite_learn_842824_3_reset( void (*free_fnc)(void* ptr) ) {
#ifdef EI_CLASSIFIER_ALLOCATION_HEAP
  free_fnc(tensor_arena);
#endif

  // scratch buffers are allocated within the arena, so just reset the counter so memory can be reused
  scratch_buffers_ix = 0;

  // overflow buffers are on the heap, so free them first
  for (size_t ix = 0; ix < overflow_buffers_ix; ix++) {
    ei_free(overflow_buffers[ix]);
  }
  overflow_buffers_ix = 0;
  return kTfLiteOk;
}
